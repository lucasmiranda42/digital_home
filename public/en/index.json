[{"content":"DeepOF is a Python suite to process animal motion tracking data, obtained using DeepLabCut or SLEAP. The tools we have when it comes to quantifying what animals do have evolved significantly in the last decade. Starting from observational research of animals in the wild, the field of ethology massively shifted towards more simplistic and controlled tests in the lab, often measuring relatively simplistic univariate responses to stimuli.\nRecently, advances is computer vision and machine learning allowed researchers to thoroughly track the detailed movement of animals both in the lab and in the wild, without the need for physical labels! Typically, these programs rely on transfer learning from neural networks pretrained on large image datasets, which first extract video frame features. Upon labelling relatively few frames with body parts of interest, the networks can then be fine-tuned to track these body parts across the entire video dataset we have. Below you can see the general workflow of the original DeepLabCut paper. More modern architectures can track keypoints even with no labelling at all, although they remain less accurate than these supervised fine-tuned methods.\nBy transforming raw video footage into time series of tracked body parts, these approaches (including the aforementioned DeepLabCut and SLEAP) paved the way for analysis tools that can automatically tag and characterize behavior in many innovative ways.\nDeepOF is our humble contribution to this endeavor. It offers both supervised and unsupervised annotation pipelines, that allow researchers to test hypotheses regarding experimental conditions such as stress, gene mutations, and sex, in a flexible way.\nThe included supervised pipeline uses a series of rule-based annotators and pre-trained machine learning models to detect when the animals that are being tracked are displaying any of a set of pre-defined behavioral patterns.\nThis way, we can tag both individual and social behaviors with just a few commands! Let\u0026rsquo;s see a basic example:\nimport deepof.data my_deepof_project_raw = deepof.data.Project( project_path=os.path.join(\u0026#34;tutorial_files\u0026#34;), video_path=os.path.join(\u0026#34;tutorial_files/Videos/\u0026#34;), table_path=os.path.join(\u0026#34;tutorial_files/Tables/\u0026#34;), project_name=\u0026#34;deepof_example_project\u0026#34;, ) supervised_annotation = my_deepof_project.supervised_annotation() And that\u0026rsquo;s it! You have now annotated your videos with a set of pre-trained models that can detect behaviors such as huddling, climbing, and social interactions.\nMoreover, DeepOF also provides an unsupervised workflow, that uses deep clustering models to uncover behavioral patterns without prior definition, which can also be run with a few lines of code:\n# Continuing from before, we create a set of coordinates to cluster, # removing unwanted positional and rotational variation coords = my_deepof_project.get_coords(center=\u0026#34;Center\u0026#34;, align=\u0026#34;Spine_1\u0026#34;) # We then preprocess these coordinates to make them suitable for clustering preprocessed_coords, global_scaler = coords.preprocess() # Finally, we cluster these preprocessed coordinates using a deep clustering model trained_model = my_deepof_project.deep_unsupervised_embedding( preprocessed_object=preprocessed_coords ) We then also include an interpretability pipeline to explore what these retrieved clusters are, relying on both Shapley Additive Explanations (SHAP) and direct mappings from clusters to video snippets. Moreover, regardless of the type of analysis you chose, DeepOF has you covered with an extensive set of post-hoc analysis and visualization tools. Below you can see the SHAP results, as well as video snippets, for a huddling cluster enriched in animals that were exposed to chronic stress.\nThat\u0026rsquo;s all for now. All in all, DeepOF is a versatile tool that can help you extract meaningful information from your animal behavior data, and we hope you find it as useful as we do!\nWhere do I go next? DeepOF is available on GitHub and PyPI. You can detailed installation instructions, as well as documentation and tutorials here. Moreover, there are two publications you can read! One about the software itself, and another one in which we used to characterize a mouse model of chronic stress!\n","permalink":"//localhost:1313/en/software/deepof/deepof/","summary":"A suite for postprocessing time-series extracted from videos of freely moving rodents using DeepLabCut and SLEAP.","title":"DeepOF"},{"content":"General impressions Back to a massive venue. This time in Vienna, Austria, for the International Conference in Machine Learning (ICML). I organised my schedule much better than last time, and I got a lot of insight on new developments in several areas I am interested in, such as time series representation learning, medical informatics, and protein computational biology. Below my humble selection of favourite papers. Hope you find something interesting!\nContrastive Learning for Clinical Outcome Prediction with Partial Data Sources We start with a significant contribution to the field of medical informatics. As electronic health records (EHR) become more and more common around the world, we see new methods to process, represent, and predict from them becoming increasignly important. In this case, the authors present CLOPPS (Contrastive Learning for clinical Outcome Prediction with Partial data Sources), which aims to capture information across different data sources from the same patients, and align them during training using contrastive learning. Importantly, they do it in a way in which not all modalities are necessary at inference time, which is a common limitation in many models.\nThe figure above represents the pretraining workflow envisioned for CLOPPS: given two longitudinal observations, the attention-based decoder on the left produces representations for each modality, which are then contrastively aligned in the latent space. The total loss used in the paper is a combination of two InfoNCE terms, that follow separate positive/negative pair sampling strategies, and a forecasting term. The first one, \\(L_M\\), is based on time matching. Here, the assumption is that different sources collectively and complementary represent health status at that moment in time, thus making samples with the same time stamp across sources the only positive positive pairs. Secondly, the model incorporates a quite original local similarity term (\\(L_L\\)), based on a Kaplan-Meier (KM) estimator for the outcome of interest. The idea is simple: if the patient is healthy, not so much changes in a short period of time, and the width of the local neighbourhood to sample positive pairs from can be larger. If a patient is sick in a particular way, the KM curve is steeper, and the neighbourhood to sample positive pairs from is smaller. While \\(L_M\\) takes care of strictly aligning the data sources across time, \\(L_L\\) focuses on regions of rapid change, corresponding with worse outcomes as predicted by the KM estimator. I think it\u0026rsquo;s a wonderful idea, which the included ablation studies show is relevant! Finally, the third term, \\(L_F\\), is a forecasting loss trained per modality independently.\nFor a review of the basic InfoNCE loss, feel free to visit this earlier post!\nWhen it comes to experiments, the authors showcase SOTA performance in a series of settings, motivated by a real-world problem where the data comes from both a dialysis provider, as well as a US-national data system –the United States Renal Data System (USRDS)–. Here, as data from the national stream is not always available, it is crucial to develop a model that can still make predictions when some data is missing.\nTimesFM: A decoder-only foundation model for time series forecasting. This is a paper that, as many other coming from Google, was all over the internet when it came out. I was really looking forward to the poster, and I\u0026rsquo;m glad I managed to pass by and chat with the authors! As the title indicates, TimesFM is a decoder-only foundation model for univariate time series forecasting, trained on both Google trends and wikimedia access statistics.\nData are tokenised across time, and the architecture matches a standard decoder-only transformer, where \u0026rsquo;tokens\u0026rsquo; are passed through a residual block instead of obtained from a lookup table (as in language models, for example) since the data is continuous. The model is trained to minimise the mean squared error loss between each token and another one, obtained at a horizon \\(h\\) in the future.\nThe figure above includes an illustration of the TimesFM model architecture during training, where an input time-series of a specific length is be broken down into input patches. As mentioned above, each patch is then processed into a vector by a residual block to the model dimension of the transformer layers. The vector is then added to sinusoidal positional encodings, and fed into a set of stacked transformer layers with multi-headed causal attention. The length of the patches and the horizon \\(h\\) vary during training, to prevent the model from overfitting to a single rigid setting.\nThe paper includes a series of experiments that explore different patching schemes on different benchmarks, reaching SOTA zero-shot performance in many of them. While this is definitely a paper to keep an eye on, one of its main limitations is the focus on univariate time series only. While training representation models on multivariate time series is a much harder problem, there are several recent works that look into this issue already. For example, iTransformer inverts the encoding strategy by tokenizing each feature independently across the entire time dimension, with self-attention focusing on feature interactions. Another interesting endeavour, which reaches SOTA performance in many benchmarks and leverages data from multiple sources, is UniTS. This model has separate attention mechanisms that work both across features and across time, and includes different sets of tokens to specify one of several tasks, including forecasting, imputation, and anomaly detection.\n2Bits of Protein: Efficient Protein Language Models at the scale of 2-Bits Finally, we shift gears a bit and move on to protein language models. This is a paper that was presented during the workshops on the last day of the conference, which builds on the recent idea of training language models with ternary weights that can be set to either zero, one, or minus one. This is a really relevant topic at the time of writing, since it has been extensively shown that models whose weights are mapped to ternary values can retain most of the precision of their full-precision counterparts, while being much more efficient in terms of memory and computation. This has huge implications for small and mid-size businesses, as well as for research labs that do not have access to the same computational resources as the big players in the field. Moreover, the ability to train and run models locally promises advantages in terms of privacy and data security, which is important in any health-related application (among others, of course).\nIn this work, the authors investigate training an encoder-only pLM using a ternary architecture, and benchmark it against ESM2 (with full-precision) using an extensive set of tasks from ProteinGym. While the ternary model is not as good as the full-precision one, it is still competitive in many tasks, and much more efficient. Authors also claim that based on the foundational work in the field, one can expect the ternary model to outperform ESM-2 once model size is scaled up. This remains to be seen!\nHere is a snippet taken from the results:\nAll in all, I think the impact of this set of approaches can be huge for both small and large players. For once, it allows actors without SOTA hardware to train and use better models, but it also allows big companies to train much larger models than they would be able to otherwise. We\u0026rsquo;ll see if these practices become mainstream in the next few years.\n","permalink":"//localhost:1313/en/blog/icml24/icml24/","summary":"My impressions, thoughts, and favourite papers on the 2024 edition of ICML, which I attended in person in Vienna.","title":"International Conference on Machine Learning 2024"},{"content":"Ever wondered how we know what we know about the brain? How scientists and doctors try (and succeed, and fail) to apply this knowledge to improve the lives of those who suffer?\nThe IMPRS-TP PsyComm group is a group of doctoral students who believe in the importance of communicating science to the general public and especially to children and adolescents, in an effort to foster scientific thinking from early on. Our goal is to educate the general public, starting from a young age, about mental health and mental illnesses. This way, we hope to raise awareness and reduce stigma associated with them.\nWith this in mind, we designed a booklet to give an overview of the fundamentals of mental health and mental health awareness. In simple terms that aim to be accessible to children, we narrate the basics of brain anatomy and function, and what we know about how mental illnesses can occur. Moreover, we delve into genetics and into the influence of the environment on our behavior, conveying that, contrary to the popular false dychotomy, both nature and nurture are important in shaping who we are.\nAll in all, the chapters in this book will help you (and/or your kids) understand the scientific principles underlying mental function, illness, and the research process.\nYou can the full English booklet here. The official German translation, as well as an unofficial Spanish version can be downloaded here and here, respectively. Enjoy!\nIf you have any questions or feedback, feel free to reach out to us at imprs-tp-scicomm@psych.mpg.de. We are always happy to hear from you!\nPersonally, I\u0026rsquo;d like to take a moment to thank each and every one of the members of the group. It was a pleasure to work with you all, and I\u0026rsquo;m proud of what we\u0026rsquo;ve accomplished together. At the moment of writing, many of us have already graduated and moved on, but I\u0026rsquo;m looking forward to seeing what the future holds for new members!\nMembers involved in writing and editing (top-left to bottom-right): Lea Brix, Anthi Krontira, Marius Stephan, Elena Brivio, Sowmya Narayan, Adyasha Kunthia, Lucas Miranda, Srivaishnavi Loganathan, Anna Fröhlich, Anna Zych, Muriel Frisch, Cassandra Deichsel, Linda Dieckmann, Mira Erhart, Nicolas Rost, and Julia Fietz. Ane Ayo Martin is not in the picture.\n","permalink":"//localhost:1313/en/blog/psycomm/psycomm_booklet/","summary":"As part of the science communication group within the Max Planck Institute of Psychiatry, we wrote and released a booklet on mental health and mental health research for children. Come and check it out!","title":"Mental health, explained for everyone"},{"content":"In 2017 I had the pleasure to join an internship program at the University of Campinas, in south-east Brazil. After completing my project and courses, I had some small amount of money left, and managed to take some days off in Cabo Frio, close to Rio de Janeiro. It was a great time! But on the very last day, while trying to climb back a cliff after a snorkelling session that had extended for a bit too long, the waves pressed me to the rocks and I cut my leg. While it didn\u0026rsquo;t seem bad in the beginning, it got badly infected and, after several weeks of trying different antibiotics (back in Buenos Aires at this point), the infection was still there. After several tries and some blood analyses, the doctors found something that worked, leaving me only with a pretty big scar and a hopefully engaging introduction to this blog post. Others are not so lucky.\nAntimicrobial resistance (AMR) is a growing concern in the medical community. The World Health Organization (WHO) has declared it a global health emergency, and [\u0026hellip;]\nInclude stats Introduce the problem in the clinic Summarise the Weis et al paper Summarise the AMR multimodal paper ","permalink":"//localhost:1313/en/blog/amr_multimodal/amr_multimodal/","summary":"Predicting antimicrobial resistance in clinical settings could have significant impact for patients. Here, we extend the current state of the art by incorporating antibiotic structural information into our models.","title":"Multimodal learning in clinical proteomics"},{"content":"General impressions Last December, I had the pleasure of attending the 2023 edition of the Neural Information Processing Systems (NeurIPS) conference in New Orleans. It was my first time in such a large venue, and I must admit I was slightly overwhelmed by the number of people and the sheer amount of information available. However, I got a lot of insight from the several interesting talks and poster sessions I managed to attend, and I would like to share some of my favourite snippets with whoever is interested. Below the three papers I found most interesting for my current research, as well as a bonus track on an amazing project that calls back to my PhD days.\nContrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series The first paper in the list introduces COMET, a contrastive learning framework to learn representations of medical time series in a self-supervised manner. The main contribution the authors present is a hierarchical training scheme for time series encoders that has four main contrastive loss terms, aiming to contrast single observations, trials, samples, and patients, exploiting and learning from different levels of data consistency that may be lost otherwise. For all levels, they use variants of the InfoNCE (Information Noise Contrastive Estimation) loss, where positive and negative pairs are sampled using a masking-based augmentation procedure.\nAs a recap, let\u0026rsquo;s start with the regular InfoNCE loss. Given a set of samples (/x_i/) and a different set /(x_j/), the InfoNCE loss is defined as:\n$$ \\mathcal{L}_{\\text{InfoNCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(x_i, x_i^+))}{\\sum_{j=1}^{K} \\exp(\\text{sim}(x_i, x_j^-))} $$where \\(x_i^+\\) is a positive sample, \\(x_j^-\\) a negative sample, and \\(\\text{sim}\\) a similarity function (typically the dot product between the embeddings or cosine similarity, but it could be anything else). The idea is to maximise the similarity between the encoder and projection head for positive pairs, while minimising it for negative pairs (notice the \\(-\\) sign!). But how do we define positive and negative pairs? Well, this is where COMET shines\u0026hellip;\nThe four levels I mentioned above are defined as follows:\nObservation level: First, augmented observations of the same time point are treated as positive pairs \\((x_{i,t}, \\tilde{x}_{i,t})\\), while real and augmented observations of different time points are treated as negative \\((x_{i,t}, x_{i,-t}\\) and \\((x_{i,t}, \\tilde{x}_{i,-t})\\), where \\(x\\) is a given sample, \\(\\tilde{x}\\) an augmented sample, \\(i\\) the sample index, and \\(t\\) the time index.\nSample level: Second, augmented observations of the same sample are treated as positive pairs \\((x_{i}, \\tilde{x}_{i})\\), while real and augmented observations of different samples are treated as negative \\((x_{i}, x_{j}\\) and \\((x_{i}, \\tilde{x}_{j})\\).\nTrial level: We now take subsets of each time series into account for memory purposes, and group observations belonging to the same subset in to trials. We apply the same basic loss, but to an aggregated representation of the entire trial instead of the observation-level embeddings. Of all four, it\u0026rsquo;s the only one that comes as an artefact of hardware limitations, rather than from real consistencies in the data themselves. It\u0026rsquo;s unclear if this would be necessary in a more powerful hardware setup.\nPatient level: last but not least, we repeat step 3, but now with trials grouped into belonging to the same patient or not. Different time series sampled from the same individuals are regarded as positive pairs, and as negative otherwise. This is really interesting because it\u0026rsquo;s the only level that takes into account the patient\u0026rsquo;s identity, which can be crucial for generalisation in medical data.\nBy training the encoder with these four levels of contrastive loss, the authors aim to learn representations that are invariant to the different levels of data consistency.\nThe main workflow looks like this (check the paper for more details!):\nThe experiments in the paper focus on regularly sampled EEG data with little to no missing values. However, their contrastive framework seems like a great starting point for multi-level learning for irregularly sample data (such as ICU time series) as well. They beat several time series self-supervised baselines in detecting myocardial infarction and Parkinson\u0026rsquo;s disease.\nTime Series as Images: Vision Transformer for Irregularly Sampled Time Series We now move to a paper that caught my attention because of the lateral thinking in place. Time Series as Images presents a peculiar way of processing irregularly sampled time series for supervised learning, based on plotting the collected values as images and processing them using a pretrained vision swin transformer. The authors run several unorthodox experiments on how different styles of plotting influence performance, such as markers, interpolation, variable order, and colours. Moreover, they include several experiments on intensive care unit data, and provide direct comparisons with many baselines, including SeFT from the Borgwardt lab where I work. They reach SOTA performance in various tasks, including sepsis and mortality prediction on physionet 2019 and 2012 challenge data, respectively. They train the models with a simple binary cross-entropy loss, while upsampling the minority class. Interestingly, they also included static information (age, height, weight, sex, demographics) as a paragraph embedded with an encoder-only language model. Time series and static text embeddings were concatenated prior to classification. Truly unorthodox, but it seems to work!\nThe main workflow looks like this (check the paper for more details!):\nAll in all, this article clearly demonstrates the generalising capabilities of large vision models, but whether their ideas can be extended to make generalizable time series models remains to be seen.\nAn Iterative Self-Learning Framework for Medical Domain Generalization Finally, the third work in this summary presents an approach to mitigate distribution shift in electronic health record (EHR) data, named SLDG (Self-Learning Domain Generalization). In a nutshell, the approach starts by grouping the features semantically in different classes (such as statics, symptoms, treatments, and medical history). Each feature subset is embedded with a trained encoder to an individual latent space, and a series of latent domains are retrieved for each modality using hierarchical clustering, with the number of clusters selected automatically based on the silhouette score. This makes it easier to unravel really specific clusters as the intersection of not-so-rare groups of specific features (such as a cluster of older male patients with a history of smoking and type 2 diabetes, which can be decomposed in older, male, smoking, and type 2 diabetes). Lastly, individual classifiers for a given target variable are trained for each of these feature classes, with clusters recomputed every 20 epochs.\nTheir experiments focus on 15-day readmission and mortality prediction on both MIMIC-IV and eICU, with data splits maximising the temporal and spatial gaps between samples, the latter based on the geographical location of the hospitals. They beat several domain generalization baselines on all tasks and metrics, which sounds promising. Interestingly, they do not provide generalization metrics between eICU and MIMIC-IV; only within each dataset individually.\nAll in all, this seems like a smart approach to efficiently group representations based on prior knowledge about the features, which can have a positive impact on domain shift.\nBonus track: AmadeusGPT: a natural language interface for interactive animal behavioral analysis During my PhD, I worked a lot with motion-tracking data coming from animal experiments. The absolute state-of-the-art, both in terms of performance and user support, for motion tracking in biology, is DeepLabCut. However, the main software is not the most user-friendly, and it can be quite cumbersome to use for wet-lab biologists. This is where AmadeusGPT comes in. By leveraging several milestones from the Mathis lab, such as DLC super animal (a model that enables zero-shot tracking), object segmentation using SAM, and API calls to ChatGPT, the authors (also the DeepLabCut team!) present a natural language interface for interactive animal behavioral analysis, where the user can ask questions about the data in plain English, and the system will return the relevant information.\nAt the time of writing, the system is available upon request (which is understandable given the pricing of large-volume ChatGPT API calls), but it seems like a great step forward in making cutting-edge technology more accessible to the general public. I can\u0026rsquo;t wait to see how this project evolves!\n","permalink":"//localhost:1313/en/blog/neurips23/neurips23/","summary":"My impressions, thoughts, and favourite papers on the 2023 edition of NeurIPS, which I attended in person in New Orleans.","title":"Neural Information Processing Systems 2023"},{"content":"","permalink":"//localhost:1313/en/pubs/liver_dvp/","summary":"","title":"Single cell spatial proteomics maps human liver zonation patterns and their vulnerability to fibrosis"},{"content":"","permalink":"//localhost:1313/en/pubs/conformal_amr/","summary":"","title":"Detecting antimicrobial resistance through MALDI-TOF mass spectrometry with statistical guarantees using conformal prediction"},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_metabolism/","summary":"","title":"Sex-specific fear acquisition following early life stress is linked to amygdala and hippocampal purine and glutamate metabolism"},{"content":"","permalink":"//localhost:1313/en/pubs/phd_thesis/","summary":"","title":"Deep clustering of animal motion tracking data: software development and applications to psychiatric preclinical research"},{"content":" You have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\nMy name is Lucas Miranda. I was born and raised in Buenos Aires, Argentina, where I studied molecular biology and bioinformatics at the University of Buenos Aires. I recently completed my doctorate at the Technical University of Munich, directed by Bertram Müller-Myhsok at the Max Planck Institute of Psychiatry. If you\u0026rsquo;re interested, you can read my thesis here.\nSince June 2023, I work as a postdoctoral researcher at the Max Planck Institute of Biochemistry in Munich, Germany, as part of the Machine Learning and Systems Biology research group. My current research focuses on representation learning of biological and medical data, with a particular interest in clinical proteomics and medical time series, including electronic health records and intensive care unit trajectories.\nWe live in exciting times, where machine learning models are becoming better by the day in many domains. My goal is to contribute my bit to the development of this technology in the medical field, which can ultimately benefit us all. Thank you for tuning in, and I hope you find something useful.\n","permalink":"//localhost:1313/en/about/","summary":"\u003chr\u003e\n\u003cp\u003eYou have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eMy name is Lucas Miranda. I was born and raised in \u003ca href=\"https://www.youtube.com/watch?v=Pb9Hv9lw5Tw\"\u003eBuenos Aires, Argentina\u003c/a\u003e, where I studied molecular biology and bioinformatics at the \u003ca href=\"https://www.uba.ar/\"\u003eUniversity of Buenos Aires\u003c/a\u003e. I recently completed my doctorate at the \u003ca href=\"https://www.tum.de/en/\"\u003eTechnical University of Munich\u003c/a\u003e, directed by Bertram Müller-Myhsok at the \u003ca href=\"https://www.psych.mpg.de/1495975/mueller_myhsok\"\u003eMax Planck Institute of Psychiatry\u003c/a\u003e. If you\u0026rsquo;re interested, you can read my thesis \u003ca href=\"https://mediatum.ub.tum.de/?id=1713444\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"About me "},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_hippocampus/","summary":"","title":"Early life adversity shapes social subordination and cell type–specific transcriptomic patterning in the ventral hippocampus"},{"content":"","permalink":"//localhost:1313/en/pubs/multimodal_amr/","summary":"","title":"Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information"},{"content":"","permalink":"//localhost:1313/en/pubs/social_beh_ns_review/","summary":"","title":"Advancing social behavioral neuroscience by integrating ethology and comparative psychology methods through machine learning"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_ncomms/","summary":"","title":"Automatically annotated motion tracking identifies a distinct social behavioral profile following chronic social defeat stress"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_joss/","summary":"","title":"DeepOF: A Python package for supervised and unsupervised pattern recognition in mice motion tracking data"},{"content":"","permalink":"//localhost:1313/en/pubs/mirnas_uba/","summary":"","title":"miRNAs associated with endoplasmic reticulum stress and unfolded protein response during decidualization"},{"content":"","permalink":"//localhost:1313/en/pubs/stress_commentary/","summary":"","title":"Increasing resolution in stress neurobiology: from single cells to complex group behaviors"},{"content":"","permalink":"//localhost:1313/en/pubs/fmri_review/","summary":"","title":"Systematic review of functional MRI applications for psychiatric disease subtyping"},{"content":"","permalink":"//localhost:1313/en/pubs/mips_uba/","summary":"","title":"Comprehensive identification of pathogenic gene variants in patients with neuroendocrine disorders"},{"content":"","permalink":"//localhost:1313/en/pubs/nfkb_uba/","summary":"","title":"The transcription factor NF-kB mediates thyrotropin-stimulated expression of thyroid differentiation markers"},{"content":"DeepOF is a Python suite to process animal motion tracking data, obtained using DeepLabCut or SLEAP. The tools we have when it comes to quantifying what animals do have evolved significantly in the last decade. Starting from observational research of animals in the wild, the field of ethology massively shifted towards more simplistic and controlled tests in the lab, often measuring relatively simplistic univariate responses to stimuli.\nRecently, advances is computer vision and machine learning allowed researchers to thoroughly track the detailed movement of animals both in the lab and in the wild, without the need for physical labels! Typically, these programs rely on transfer learning from neural networks pretrained on large image datasets, which first extract video frame features. Upon labelling relatively few frames with body parts of interest, the networks can then be fine-tuned to track these body parts across the entire video dataset we have. Below you can see the general workflow of the original DeepLabCut paper. More modern architectures can track keypoints even with no labelling at all, although they remain less accurate than these supervised fine-tuned methods.\nBy transforming raw video footage into time series of tracked body parts, these approaches (including the aforementioned DeepLabCut and SLEAP) paved the way for analysis tools that can automatically tag and characterize behavior in many innovative ways.\nDeepOF is our humble contribution to this endeavor. It offers both supervised and unsupervised annotation pipelines, that allow researchers to test hypotheses regarding experimental conditions such as stress, gene mutations, and sex, in a flexible way.\nThe included supervised pipeline uses a series of rule-based annotators and pre-trained machine learning models to detect when the animals that are being tracked are displaying any of a set of pre-defined behavioral patterns.\nThis way, we can tag both individual and social behaviors with just a few commands! Let\u0026rsquo;s see a basic example:\nimport deepof.data my_deepof_project_raw = deepof.data.Project( project_path=os.path.join(\u0026#34;tutorial_files\u0026#34;), video_path=os.path.join(\u0026#34;tutorial_files/Videos/\u0026#34;), table_path=os.path.join(\u0026#34;tutorial_files/Tables/\u0026#34;), project_name=\u0026#34;deepof_example_project\u0026#34;, ) supervised_annotation = my_deepof_project.supervised_annotation() And that\u0026rsquo;s it! You have now annotated your videos with a set of pre-trained models that can detect behaviors such as huddling, climbing, and social interactions.\nMoreover, DeepOF also provides an unsupervised workflow, that uses deep clustering models to uncover behavioral patterns without prior definition, which can also be run with a few lines of code:\n# Continuing from before, we create a set of coordinates to cluster, # removing unwanted positional and rotational variation coords = my_deepof_project.get_coords(center=\u0026#34;Center\u0026#34;, align=\u0026#34;Spine_1\u0026#34;) # We then preprocess these coordinates to make them suitable for clustering preprocessed_coords, global_scaler = coords.preprocess() # Finally, we cluster these preprocessed coordinates using a deep clustering model trained_model = my_deepof_project.deep_unsupervised_embedding( preprocessed_object=preprocessed_coords ) We then also include an interpretability pipeline to explore what these retrieved clusters are, relying on both Shapley Additive Explanations (SHAP) and direct mappings from clusters to video snippets. Moreover, regardless of the type of analysis you chose, DeepOF has you covered with an extensive set of post-hoc analysis and visualization tools. Below you can see the SHAP results, as well as video snippets, for a huddling cluster enriched in animals that were exposed to chronic stress.\nThat\u0026rsquo;s all for now. All in all, DeepOF is a versatile tool that can help you extract meaningful information from your animal behavior data, and we hope you find it as useful as we do!\nWhere do I go next? DeepOF is available on GitHub and PyPI. You can detailed installation instructions, as well as documentation and tutorials here. Moreover, there are two publications you can read! One about the software itself, and another one in which we used to characterize a mouse model of chronic stress!\n","permalink":"//localhost:1313/en/software/deepof/deepof/","summary":"A suite for postprocessing time-series extracted from videos of freely moving rodents using DeepLabCut and SLEAP.","title":"DeepOF"},{"content":"General impressions Back to a massive venue. This time in Vienna, Austria, for the International Conference in Machine Learning (ICML). I organised my schedule much better than last time, and I got a lot of insight on new developments in several areas I am interested in, such as time series representation learning, medical informatics, and protein computational biology. Below my humble selection of favourite papers. Hope you find something interesting!\nContrastive Learning for Clinical Outcome Prediction with Partial Data Sources We start with a significant contribution to the field of medical informatics. As electronic health records (EHR) become more and more common around the world, we see new methods to process, represent, and predict from them becoming increasignly important. In this case, the authors present CLOPPS (Contrastive Learning for clinical Outcome Prediction with Partial data Sources), which aims to capture information across different data sources from the same patients, and align them during training using contrastive learning. Importantly, they do it in a way in which not all modalities are necessary at inference time, which is a common limitation in many models.\nThe figure above represents the pretraining workflow envisioned for CLOPPS: given two longitudinal observations, the attention-based decoder on the left produces representations for each modality, which are then contrastively aligned in the latent space. The total loss used in the paper is a combination of two InfoNCE terms, that follow separate positive/negative pair sampling strategies, and a forecasting term. The first one, \\(L_M\\), is based on time matching. Here, the assumption is that different sources collectively and complementary represent health status at that moment in time, thus making samples with the same time stamp across sources the only positive positive pairs. Secondly, the model incorporates a quite original local similarity term (\\(L_L\\)), based on a Kaplan-Meier (KM) estimator for the outcome of interest. The idea is simple: if the patient is healthy, not so much changes in a short period of time, and the width of the local neighbourhood to sample positive pairs from can be larger. If a patient is sick in a particular way, the KM curve is steeper, and the neighbourhood to sample positive pairs from is smaller. While \\(L_M\\) takes care of strictly aligning the data sources across time, \\(L_L\\) focuses on regions of rapid change, corresponding with worse outcomes as predicted by the KM estimator. I think it\u0026rsquo;s a wonderful idea, which the included ablation studies show is relevant! Finally, the third term, \\(L_F\\), is a forecasting loss trained per modality independently.\nFor a review of the basic InfoNCE loss, feel free to visit this earlier post!\nWhen it comes to experiments, the authors showcase SOTA performance in a series of settings, motivated by a real-world problem where the data comes from both a dialysis provider, as well as a US-national data system –the United States Renal Data System (USRDS)–. Here, as data from the national stream is not always available, it is crucial to develop a model that can still make predictions when some data is missing.\nTimesFM: A decoder-only foundation model for time series forecasting. This is a paper that, as many other coming from Google, was all over the internet when it came out. I was really looking forward to the poster, and I\u0026rsquo;m glad I managed to pass by and chat with the authors! As the title indicates, TimesFM is a decoder-only foundation model for univariate time series forecasting, trained on both Google trends and wikimedia access statistics.\nData are tokenised across time, and the architecture matches a standard decoder-only transformer, where \u0026rsquo;tokens\u0026rsquo; are passed through a residual block instead of obtained from a lookup table (as in language models, for example) since the data is continuous. The model is trained to minimise the mean squared error loss between each token and another one, obtained at a horizon \\(h\\) in the future.\nThe figure above includes an illustration of the TimesFM model architecture during training, where an input time-series of a specific length is be broken down into input patches. As mentioned above, each patch is then processed into a vector by a residual block to the model dimension of the transformer layers. The vector is then added to sinusoidal positional encodings, and fed into a set of stacked transformer layers with multi-headed causal attention. The length of the patches and the horizon \\(h\\) vary during training, to prevent the model from overfitting to a single rigid setting.\nThe paper includes a series of experiments that explore different patching schemes on different benchmarks, reaching SOTA zero-shot performance in many of them. While this is definitely a paper to keep an eye on, one of its main limitations is the focus on univariate time series only. While training representation models on multivariate time series is a much harder problem, there are several recent works that look into this issue already. For example, iTransformer inverts the encoding strategy by tokenizing each feature independently across the entire time dimension, with self-attention focusing on feature interactions. Another interesting endeavour, which reaches SOTA performance in many benchmarks and leverages data from multiple sources, is UniTS. This model has separate attention mechanisms that work both across features and across time, and includes different sets of tokens to specify one of several tasks, including forecasting, imputation, and anomaly detection.\n2Bits of Protein: Efficient Protein Language Models at the scale of 2-Bits Finally, we shift gears a bit and move on to protein language models. This is a paper that was presented during the workshops on the last day of the conference, which builds on the recent idea of training language models with ternary weights that can be set to either zero, one, or minus one. This is a really relevant topic at the time of writing, since it has been extensively shown that models whose weights are mapped to ternary values can retain most of the precision of their full-precision counterparts, while being much more efficient in terms of memory and computation. This has huge implications for small and mid-size businesses, as well as for research labs that do not have access to the same computational resources as the big players in the field. Moreover, the ability to train and run models locally promises advantages in terms of privacy and data security, which is important in any health-related application (among others, of course).\nIn this work, the authors investigate training an encoder-only pLM using a ternary architecture, and benchmark it against ESM2 (with full-precision) using an extensive set of tasks from ProteinGym. While the ternary model is not as good as the full-precision one, it is still competitive in many tasks, and much more efficient. Authors also claim that based on the foundational work in the field, one can expect the ternary model to outperform ESM-2 once model size is scaled up. This remains to be seen!\nHere is a snippet taken from the results:\nAll in all, I think the impact of this set of approaches can be huge for both small and large players. For once, it allows actors without SOTA hardware to train and use better models, but it also allows big companies to train much larger models than they would be able to otherwise. We\u0026rsquo;ll see if these practices become mainstream in the next few years.\n","permalink":"//localhost:1313/en/blog/icml24/icml24/","summary":"My impressions, thoughts, and favourite papers on the 2024 edition of ICML, which I attended in person in Vienna.","title":"International Conference on Machine Learning 2024"},{"content":"Ever wondered how we know what we know about the brain? How scientists and doctors try (and succeed, and fail) to apply this knowledge to improve the lives of those who suffer?\nThe IMPRS-TP PsyComm group is a group of doctoral students who believe in the importance of communicating science to the general public and especially to children and adolescents, in an effort to foster scientific thinking from early on. Our goal is to educate the general public, starting from a young age, about mental health and mental illnesses. This way, we hope to raise awareness and reduce stigma associated with them.\nWith this in mind, we designed a booklet to give an overview of the fundamentals of mental health and mental health awareness. In simple terms that aim to be accessible to children, we narrate the basics of brain anatomy and function, and what we know about how mental illnesses can occur. Moreover, we delve into genetics and into the influence of the environment on our behavior, conveying that, contrary to the popular false dychotomy, both nature and nurture are important in shaping who we are.\nAll in all, the chapters in this book will help you (and/or your kids) understand the scientific principles underlying mental function, illness, and the research process.\nYou can the full English booklet here. The official German translation, as well as an unofficial Spanish version can be downloaded here and here, respectively. Enjoy!\nIf you have any questions or feedback, feel free to reach out to us at imprs-tp-scicomm@psych.mpg.de. We are always happy to hear from you!\nPersonally, I\u0026rsquo;d like to take a moment to thank each and every one of the members of the group. It was a pleasure to work with you all, and I\u0026rsquo;m proud of what we\u0026rsquo;ve accomplished together. At the moment of writing, many of us have already graduated and moved on, but I\u0026rsquo;m looking forward to seeing what the future holds for new members!\nMembers involved in writing and editing (top-left to bottom-right): Lea Brix, Anthi Krontira, Marius Stephan, Elena Brivio, Sowmya Narayan, Adyasha Kunthia, Lucas Miranda, Srivaishnavi Loganathan, Anna Fröhlich, Anna Zych, Muriel Frisch, Cassandra Deichsel, Linda Dieckmann, Mira Erhart, Nicolas Rost, and Julia Fietz. Ane Ayo Martin is not in the picture.\n","permalink":"//localhost:1313/en/blog/psycomm/psycomm_booklet/","summary":"As part of the science communication group within the Max Planck Institute of Psychiatry, we wrote and released a booklet on mental health and mental health research for children. Come and check it out!","title":"Mental health, explained for everyone"},{"content":"In 2017 I had the pleasure to join an internship program at the University of Campinas, in south-east Brazil. After completing my project and courses, I had some small amount of money left, and managed to take some days off in Cabo Frio, close to Rio de Janeiro. It was a great time! But on the very last day, while trying to climb back a cliff after a snorkelling session that had extended for a bit too long, the waves pressed me to the rocks and I cut my leg. While it didn\u0026rsquo;t seem bad in the beginning, it got badly infected and, after several weeks of trying different antibiotics (back in Buenos Aires at this point), the infection was still there. After several tries and some blood analyses, the doctors found something that worked, leaving me only with a pretty big scar and a hopefully engaging introduction to this blog post. Others are not so lucky.\nAntimicrobial resistance (AMR) is a growing concern in the medical community. The World Health Organization (WHO) has declared it a global health emergency, and [\u0026hellip;]\nInclude stats Introduce the problem in the clinic Summarise the Weis et al paper Summarise the AMR multimodal paper ","permalink":"//localhost:1313/en/blog/amr_multimodal/amr_multimodal/","summary":"Predicting antimicrobial resistance in clinical settings could have significant impact for patients. Here, we extend the current state of the art by incorporating antibiotic structural information into our models.","title":"Multimodal learning in clinical proteomics"},{"content":"General impressions Last December, I had the pleasure of attending the 2023 edition of the Neural Information Processing Systems (NeurIPS) conference in New Orleans. It was my first time in such a large venue, and I must admit I was slightly overwhelmed by the number of people and the sheer amount of information available. However, I got a lot of insight from the several interesting talks and poster sessions I managed to attend, and I would like to share some of my favourite snippets with whoever is interested. Below the three papers I found most interesting for my current research, as well as a bonus track on an amazing project that calls back to my PhD days.\nContrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series The first paper in the list introduces COMET, a contrastive learning framework to learn representations of medical time series in a self-supervised manner. The main contribution the authors present is a hierarchical training scheme for time series encoders that has four main contrastive loss terms, aiming to contrast single observations, trials, samples, and patients, exploiting and learning from different levels of data consistency that may be lost otherwise. For all levels, they use variants of the InfoNCE (Information Noise Contrastive Estimation) loss, where positive and negative pairs are sampled using a masking-based augmentation procedure.\nAs a recap, let\u0026rsquo;s start with the regular InfoNCE loss. Given a set of samples (/x_i/) and a different set /(x_j/), the InfoNCE loss is defined as:\n$$ \\mathcal{L}_{\\text{InfoNCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(x_i, x_i^+))}{\\sum_{j=1}^{K} \\exp(\\text{sim}(x_i, x_j^-))} $$where \\(x_i^+\\) is a positive sample, \\(x_j^-\\) a negative sample, and \\(\\text{sim}\\) a similarity function (typically the dot product between the embeddings or cosine similarity, but it could be anything else). The idea is to maximise the similarity between the encoder and projection head for positive pairs, while minimising it for negative pairs (notice the \\(-\\) sign!). But how do we define positive and negative pairs? Well, this is where COMET shines\u0026hellip;\nThe four levels I mentioned above are defined as follows:\nObservation level: First, augmented observations of the same time point are treated as positive pairs \\((x_{i,t}, \\tilde{x}_{i,t})\\), while real and augmented observations of different time points are treated as negative \\((x_{i,t}, x_{i,-t}\\) and \\((x_{i,t}, \\tilde{x}_{i,-t})\\), where \\(x\\) is a given sample, \\(\\tilde{x}\\) an augmented sample, \\(i\\) the sample index, and \\(t\\) the time index.\nSample level: Second, augmented observations of the same sample are treated as positive pairs \\((x_{i}, \\tilde{x}_{i})\\), while real and augmented observations of different samples are treated as negative \\((x_{i}, x_{j}\\) and \\((x_{i}, \\tilde{x}_{j})\\).\nTrial level: We now take subsets of each time series into account for memory purposes, and group observations belonging to the same subset in to trials. We apply the same basic loss, but to an aggregated representation of the entire trial instead of the observation-level embeddings. Of all four, it\u0026rsquo;s the only one that comes as an artefact of hardware limitations, rather than from real consistencies in the data themselves. It\u0026rsquo;s unclear if this would be necessary in a more powerful hardware setup.\nPatient level: last but not least, we repeat step 3, but now with trials grouped into belonging to the same patient or not. Different time series sampled from the same individuals are regarded as positive pairs, and as negative otherwise. This is really interesting because it\u0026rsquo;s the only level that takes into account the patient\u0026rsquo;s identity, which can be crucial for generalisation in medical data.\nBy training the encoder with these four levels of contrastive loss, the authors aim to learn representations that are invariant to the different levels of data consistency.\nThe main workflow looks like this (check the paper for more details!):\nThe experiments in the paper focus on regularly sampled EEG data with little to no missing values. However, their contrastive framework seems like a great starting point for multi-level learning for irregularly sample data (such as ICU time series) as well. They beat several time series self-supervised baselines in detecting myocardial infarction and Parkinson\u0026rsquo;s disease.\nTime Series as Images: Vision Transformer for Irregularly Sampled Time Series We now move to a paper that caught my attention because of the lateral thinking in place. Time Series as Images presents a peculiar way of processing irregularly sampled time series for supervised learning, based on plotting the collected values as images and processing them using a pretrained vision swin transformer. The authors run several unorthodox experiments on how different styles of plotting influence performance, such as markers, interpolation, variable order, and colours. Moreover, they include several experiments on intensive care unit data, and provide direct comparisons with many baselines, including SeFT from the Borgwardt lab where I work. They reach SOTA performance in various tasks, including sepsis and mortality prediction on physionet 2019 and 2012 challenge data, respectively. They train the models with a simple binary cross-entropy loss, while upsampling the minority class. Interestingly, they also included static information (age, height, weight, sex, demographics) as a paragraph embedded with an encoder-only language model. Time series and static text embeddings were concatenated prior to classification. Truly unorthodox, but it seems to work!\nThe main workflow looks like this (check the paper for more details!):\nAll in all, this article clearly demonstrates the generalising capabilities of large vision models, but whether their ideas can be extended to make generalizable time series models remains to be seen.\nAn Iterative Self-Learning Framework for Medical Domain Generalization Finally, the third work in this summary presents an approach to mitigate distribution shift in electronic health record (EHR) data, named SLDG (Self-Learning Domain Generalization). In a nutshell, the approach starts by grouping the features semantically in different classes (such as statics, symptoms, treatments, and medical history). Each feature subset is embedded with a trained encoder to an individual latent space, and a series of latent domains are retrieved for each modality using hierarchical clustering, with the number of clusters selected automatically based on the silhouette score. This makes it easier to unravel really specific clusters as the intersection of not-so-rare groups of specific features (such as a cluster of older male patients with a history of smoking and type 2 diabetes, which can be decomposed in older, male, smoking, and type 2 diabetes). Lastly, individual classifiers for a given target variable are trained for each of these feature classes, with clusters recomputed every 20 epochs.\nTheir experiments focus on 15-day readmission and mortality prediction on both MIMIC-IV and eICU, with data splits maximising the temporal and spatial gaps between samples, the latter based on the geographical location of the hospitals. They beat several domain generalization baselines on all tasks and metrics, which sounds promising. Interestingly, they do not provide generalization metrics between eICU and MIMIC-IV; only within each dataset individually.\nAll in all, this seems like a smart approach to efficiently group representations based on prior knowledge about the features, which can have a positive impact on domain shift.\nBonus track: AmadeusGPT: a natural language interface for interactive animal behavioral analysis During my PhD, I worked a lot with motion-tracking data coming from animal experiments. The absolute state-of-the-art, both in terms of performance and user support, for motion tracking in biology, is DeepLabCut. However, the main software is not the most user-friendly, and it can be quite cumbersome to use for wet-lab biologists. This is where AmadeusGPT comes in. By leveraging several milestones from the Mathis lab, such as DLC super animal (a model that enables zero-shot tracking), object segmentation using SAM, and API calls to ChatGPT, the authors (also the DeepLabCut team!) present a natural language interface for interactive animal behavioral analysis, where the user can ask questions about the data in plain English, and the system will return the relevant information.\nAt the time of writing, the system is available upon request (which is understandable given the pricing of large-volume ChatGPT API calls), but it seems like a great step forward in making cutting-edge technology more accessible to the general public. I can\u0026rsquo;t wait to see how this project evolves!\n","permalink":"//localhost:1313/en/blog/neurips23/neurips23/","summary":"My impressions, thoughts, and favourite papers on the 2023 edition of NeurIPS, which I attended in person in New Orleans.","title":"Neural Information Processing Systems 2023"},{"content":"","permalink":"//localhost:1313/en/pubs/liver_dvp/","summary":"","title":"Single cell spatial proteomics maps human liver zonation patterns and their vulnerability to fibrosis"},{"content":"","permalink":"//localhost:1313/en/pubs/conformal_amr/","summary":"","title":"Detecting antimicrobial resistance through MALDI-TOF mass spectrometry with statistical guarantees using conformal prediction"},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_metabolism/","summary":"","title":"Sex-specific fear acquisition following early life stress is linked to amygdala and hippocampal purine and glutamate metabolism"},{"content":"","permalink":"//localhost:1313/en/pubs/phd_thesis/","summary":"","title":"Deep clustering of animal motion tracking data: software development and applications to psychiatric preclinical research"},{"content":" You have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\nMy name is Lucas Miranda. I was born and raised in Buenos Aires, Argentina, where I studied molecular biology and bioinformatics at the University of Buenos Aires. I recently completed my doctorate at the Technical University of Munich, directed by Bertram Müller-Myhsok at the Max Planck Institute of Psychiatry. If you\u0026rsquo;re interested, you can read my thesis here.\nSince June 2023, I work as a postdoctoral researcher at the Max Planck Institute of Biochemistry in Munich, Germany, as part of the Machine Learning and Systems Biology research group. My current research focuses on representation learning of biological and medical data, with a particular interest in clinical proteomics and medical time series, including electronic health records and intensive care unit trajectories.\nWe live in exciting times, where machine learning models are becoming better by the day in many domains. My goal is to contribute my bit to the development of this technology in the medical field, which can ultimately benefit us all. Thank you for tuning in, and I hope you find something useful.\n","permalink":"//localhost:1313/en/about/","summary":"\u003chr\u003e\n\u003cp\u003eYou have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eMy name is Lucas Miranda. I was born and raised in \u003ca href=\"https://www.youtube.com/watch?v=Pb9Hv9lw5Tw\"\u003eBuenos Aires, Argentina\u003c/a\u003e, where I studied molecular biology and bioinformatics at the \u003ca href=\"https://www.uba.ar/\"\u003eUniversity of Buenos Aires\u003c/a\u003e. I recently completed my doctorate at the \u003ca href=\"https://www.tum.de/en/\"\u003eTechnical University of Munich\u003c/a\u003e, directed by Bertram Müller-Myhsok at the \u003ca href=\"https://www.psych.mpg.de/1495975/mueller_myhsok\"\u003eMax Planck Institute of Psychiatry\u003c/a\u003e. If you\u0026rsquo;re interested, you can read my thesis \u003ca href=\"https://mediatum.ub.tum.de/?id=1713444\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"About me "},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_hippocampus/","summary":"","title":"Early life adversity shapes social subordination and cell type–specific transcriptomic patterning in the ventral hippocampus"},{"content":"","permalink":"//localhost:1313/en/pubs/multimodal_amr/","summary":"","title":"Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information"},{"content":"","permalink":"//localhost:1313/en/pubs/social_beh_ns_review/","summary":"","title":"Advancing social behavioral neuroscience by integrating ethology and comparative psychology methods through machine learning"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_ncomms/","summary":"","title":"Automatically annotated motion tracking identifies a distinct social behavioral profile following chronic social defeat stress"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_joss/","summary":"","title":"DeepOF: A Python package for supervised and unsupervised pattern recognition in mice motion tracking data"},{"content":"","permalink":"//localhost:1313/en/pubs/mirnas_uba/","summary":"","title":"miRNAs associated with endoplasmic reticulum stress and unfolded protein response during decidualization"},{"content":"","permalink":"//localhost:1313/en/pubs/stress_commentary/","summary":"","title":"Increasing resolution in stress neurobiology: from single cells to complex group behaviors"},{"content":"","permalink":"//localhost:1313/en/pubs/fmri_review/","summary":"","title":"Systematic review of functional MRI applications for psychiatric disease subtyping"},{"content":"","permalink":"//localhost:1313/en/pubs/mips_uba/","summary":"","title":"Comprehensive identification of pathogenic gene variants in patients with neuroendocrine disorders"},{"content":"","permalink":"//localhost:1313/en/pubs/nfkb_uba/","summary":"","title":"The transcription factor NF-kB mediates thyrotropin-stimulated expression of thyroid differentiation markers"},{"content":"DeepOF is a Python suite to process animal motion tracking data, obtained using DeepLabCut or SLEAP. The tools we have when it comes to quantifying what animals do have evolved significantly in the last decade. Starting from observational research of animals in the wild, the field of ethology massively shifted towards more simplistic and controlled tests in the lab, often measuring relatively simplistic univariate responses to stimuli.\nRecently, advances is computer vision and machine learning allowed researchers to thoroughly track the detailed movement of animals both in the lab and in the wild, without the need for physical labels! Typically, these programs rely on transfer learning from neural networks pretrained on large image datasets, which first extract video frame features. Upon labelling relatively few frames with body parts of interest, the networks can then be fine-tuned to track these body parts across the entire video dataset we have. Below you can see the general workflow of the original DeepLabCut paper. More modern architectures can track keypoints even with no labelling at all, although they remain less accurate than these supervised fine-tuned methods.\nBy transforming raw video footage into time series of tracked body parts, these approaches (including the aforementioned DeepLabCut and SLEAP) paved the way for analysis tools that can automatically tag and characterize behavior in many innovative ways.\nDeepOF is our humble contribution to this endeavor. It offers both supervised and unsupervised annotation pipelines, that allow researchers to test hypotheses regarding experimental conditions such as stress, gene mutations, and sex, in a flexible way.\nThe included supervised pipeline uses a series of rule-based annotators and pre-trained machine learning models to detect when the animals that are being tracked are displaying any of a set of pre-defined behavioral patterns.\nThis way, we can tag both individual and social behaviors with just a few commands! Let\u0026rsquo;s see a basic example:\nimport deepof.data my_deepof_project_raw = deepof.data.Project( project_path=os.path.join(\u0026#34;tutorial_files\u0026#34;), video_path=os.path.join(\u0026#34;tutorial_files/Videos/\u0026#34;), table_path=os.path.join(\u0026#34;tutorial_files/Tables/\u0026#34;), project_name=\u0026#34;deepof_example_project\u0026#34;, ) supervised_annotation = my_deepof_project.supervised_annotation() And that\u0026rsquo;s it! You have now annotated your videos with a set of pre-trained models that can detect behaviors such as huddling, climbing, and social interactions.\nMoreover, DeepOF also provides an unsupervised workflow, that uses deep clustering models to uncover behavioral patterns without prior definition, which can also be run with a few lines of code:\n# Continuing from before, we create a set of coordinates to cluster, # removing unwanted positional and rotational variation coords = my_deepof_project.get_coords(center=\u0026#34;Center\u0026#34;, align=\u0026#34;Spine_1\u0026#34;) # We then preprocess these coordinates to make them suitable for clustering preprocessed_coords, global_scaler = coords.preprocess() # Finally, we cluster these preprocessed coordinates using a deep clustering model trained_model = my_deepof_project.deep_unsupervised_embedding( preprocessed_object=preprocessed_coords ) We then also include an interpretability pipeline to explore what these retrieved clusters are, relying on both Shapley Additive Explanations (SHAP) and direct mappings from clusters to video snippets. Moreover, regardless of the type of analysis you chose, DeepOF has you covered with an extensive set of post-hoc analysis and visualization tools. Below you can see the SHAP results, as well as video snippets, for a huddling cluster enriched in animals that were exposed to chronic stress.\nThat\u0026rsquo;s all for now. All in all, DeepOF is a versatile tool that can help you extract meaningful information from your animal behavior data, and we hope you find it as useful as we do!\nWhere do I go next? DeepOF is available on GitHub and PyPI. You can detailed installation instructions, as well as documentation and tutorials here. Moreover, there are two publications you can read! One about the software itself, and another one in which we used to characterize a mouse model of chronic stress!\n","permalink":"//localhost:1313/en/software/deepof/deepof/","summary":"A suite for postprocessing time-series extracted from videos of freely moving rodents using DeepLabCut and SLEAP.","title":"DeepOF"},{"content":"General impressions Back to a massive venue. This time in Vienna, Austria, for the International Conference in Machine Learning (ICML). I organised my schedule much better than last time, and I got a lot of insight on new developments in several areas I am interested in, such as time series representation learning, medical informatics, and protein computational biology. Below my humble selection of favourite papers. Hope you find something interesting!\nContrastive Learning for Clinical Outcome Prediction with Partial Data Sources We start with a significant contribution to the field of medical informatics. As electronic health records (EHR) become more and more common around the world, we see new methods to process, represent, and predict from them becoming increasignly important. In this case, the authors present CLOPPS (Contrastive Learning for clinical Outcome Prediction with Partial data Sources), which aims to capture information across different data sources from the same patients, and align them during training using contrastive learning. Importantly, they do it in a way in which not all modalities are necessary at inference time, which is a common limitation in many models.\nThe figure above represents the pretraining workflow envisioned for CLOPPS: given two longitudinal observations, the attention-based decoder on the left produces representations for each modality, which are then contrastively aligned in the latent space. The total loss used in the paper is a combination of two InfoNCE terms, that follow separate positive/negative pair sampling strategies, and a forecasting term. The first one, \\(L_M\\), is based on time matching. Here, the assumption is that different sources collectively and complementary represent health status at that moment in time, thus making samples with the same time stamp across sources the only positive positive pairs. Secondly, the model incorporates a quite original local similarity term (\\(L_L\\)), based on a Kaplan-Meier (KM) estimator for the outcome of interest. The idea is simple: if the patient is healthy, not so much changes in a short period of time, and the width of the local neighbourhood to sample positive pairs from can be larger. If a patient is sick in a particular way, the KM curve is steeper, and the neighbourhood to sample positive pairs from is smaller. While \\(L_M\\) takes care of strictly aligning the data sources across time, \\(L_L\\) focuses on regions of rapid change, corresponding with worse outcomes as predicted by the KM estimator. I think it\u0026rsquo;s a wonderful idea, which the included ablation studies show is relevant! Finally, the third term, \\(L_F\\), is a forecasting loss trained per modality independently.\nFor a review of the basic InfoNCE loss, feel free to visit this earlier post!\nWhen it comes to experiments, the authors showcase SOTA performance in a series of settings, motivated by a real-world problem where the data comes from both a dialysis provider, as well as a US-national data system –the United States Renal Data System (USRDS)–. Here, as data from the national stream is not always available, it is crucial to develop a model that can still make predictions when some data is missing.\nTimesFM: A decoder-only foundation model for time series forecasting. This is a paper that, as many other coming from Google, was all over the internet when it came out. I was really looking forward to the poster, and I\u0026rsquo;m glad I managed to pass by and chat with the authors! As the title indicates, TimesFM is a decoder-only foundation model for univariate time series forecasting, trained on both Google trends and wikimedia access statistics.\nData are tokenised across time, and the architecture matches a standard decoder-only transformer, where \u0026rsquo;tokens\u0026rsquo; are passed through a residual block instead of obtained from a lookup table (as in language models, for example) since the data is continuous. The model is trained to minimise the mean squared error loss between each token and another one, obtained at a horizon \\(h\\) in the future.\nThe figure above includes an illustration of the TimesFM model architecture during training, where an input time-series of a specific length is be broken down into input patches. As mentioned above, each patch is then processed into a vector by a residual block to the model dimension of the transformer layers. The vector is then added to sinusoidal positional encodings, and fed into a set of stacked transformer layers with multi-headed causal attention. The length of the patches and the horizon \\(h\\) vary during training, to prevent the model from overfitting to a single rigid setting.\nThe paper includes a series of experiments that explore different patching schemes on different benchmarks, reaching SOTA zero-shot performance in many of them. While this is definitely a paper to keep an eye on, one of its main limitations is the focus on univariate time series only. While training representation models on multivariate time series is a much harder problem, there are several recent works that look into this issue already. For example, iTransformer inverts the encoding strategy by tokenizing each feature independently across the entire time dimension, with self-attention focusing on feature interactions. Another interesting endeavour, which reaches SOTA performance in many benchmarks and leverages data from multiple sources, is UniTS. This model has separate attention mechanisms that work both across features and across time, and includes different sets of tokens to specify one of several tasks, including forecasting, imputation, and anomaly detection.\n2Bits of Protein: Efficient Protein Language Models at the scale of 2-Bits Finally, we shift gears a bit and move on to protein language models. This is a paper that was presented during the workshops on the last day of the conference, which builds on the recent idea of training language models with ternary weights that can be set to either zero, one, or minus one. This is a really relevant topic at the time of writing, since it has been extensively shown that models whose weights are mapped to ternary values can retain most of the precision of their full-precision counterparts, while being much more efficient in terms of memory and computation. This has huge implications for small and mid-size businesses, as well as for research labs that do not have access to the same computational resources as the big players in the field. Moreover, the ability to train and run models locally promises advantages in terms of privacy and data security, which is important in any health-related application (among others, of course).\nIn this work, the authors investigate training an encoder-only pLM using a ternary architecture, and benchmark it against ESM2 (with full-precision) using an extensive set of tasks from ProteinGym. While the ternary model is not as good as the full-precision one, it is still competitive in many tasks, and much more efficient. Authors also claim that based on the foundational work in the field, one can expect the ternary model to outperform ESM-2 once model size is scaled up. This remains to be seen!\nHere is a snippet taken from the results:\nAll in all, I think the impact of this set of approaches can be huge for both small and large players. For once, it allows actors without SOTA hardware to train and use better models, but it also allows big companies to train much larger models than they would be able to otherwise. We\u0026rsquo;ll see if these practices become mainstream in the next few years.\n","permalink":"//localhost:1313/en/blog/icml24/icml24/","summary":"My impressions, thoughts, and favourite papers on the 2024 edition of ICML, which I attended in person in Vienna.","title":"International Conference on Machine Learning 2024"},{"content":"Ever wondered how we know what we know about the brain? How scientists and doctors try (and succeed, and fail) to apply this knowledge to improve the lives of those who suffer?\nThe IMPRS-TP PsyComm group is a group of doctoral students who believe in the importance of communicating science to the general public and especially to children and adolescents, in an effort to foster scientific thinking from early on. Our goal is to educate the general public, starting from a young age, about mental health and mental illnesses. This way, we hope to raise awareness and reduce stigma associated with them.\nWith this in mind, we designed a booklet to give an overview of the fundamentals of mental health and mental health awareness. In simple terms that aim to be accessible to children, we narrate the basics of brain anatomy and function, and what we know about how mental illnesses can occur. Moreover, we delve into genetics and into the influence of the environment on our behavior, conveying that, contrary to the popular false dychotomy, both nature and nurture are important in shaping who we are.\nAll in all, the chapters in this book will help you (and/or your kids) understand the scientific principles underlying mental function, illness, and the research process.\nYou can the full English booklet here. The official German translation, as well as an unofficial Spanish version can be downloaded here and here, respectively. Enjoy!\nIf you have any questions or feedback, feel free to reach out to us at imprs-tp-scicomm@psych.mpg.de. We are always happy to hear from you!\nPersonally, I\u0026rsquo;d like to take a moment to thank each and every one of the members of the group. It was a pleasure to work with you all, and I\u0026rsquo;m proud of what we\u0026rsquo;ve accomplished together. At the moment of writing, many of us have already graduated and moved on, but I\u0026rsquo;m looking forward to seeing what the future holds for new members!\nMembers involved in writing and editing (top-left to bottom-right): Lea Brix, Anthi Krontira, Marius Stephan, Elena Brivio, Sowmya Narayan, Adyasha Kunthia, Lucas Miranda, Srivaishnavi Loganathan, Anna Fröhlich, Anna Zych, Muriel Frisch, Cassandra Deichsel, Linda Dieckmann, Mira Erhart, Nicolas Rost, and Julia Fietz. Ane Ayo Martin is not in the picture.\n","permalink":"//localhost:1313/en/blog/psycomm/psycomm_booklet/","summary":"As part of the science communication group within the Max Planck Institute of Psychiatry, we wrote and released a booklet on mental health and mental health research for children. Come and check it out!","title":"Mental health, explained for everyone"},{"content":"In 2017 I had the pleasure to join an internship program at the University of Campinas, in south-east Brazil. After completing my project and courses, I had some small amount of money left, and managed to take some days off in Cabo Frio, close to Rio de Janeiro. It was a great time! But on the very last day, while trying to climb back a cliff after a snorkelling session that had extended for a bit too long, the waves pressed me to the rocks and I cut my leg. While it didn\u0026rsquo;t seem bad in the beginning, it got badly infected and, after several weeks of trying different antibiotics (back in Buenos Aires at this point), the infection was still there. After several tries and some blood analyses, the doctors found something that worked, leaving me only with a pretty big scar and a hopefully engaging introduction to this blog post. Others are not so lucky.\nAntimicrobial resistance (AMR) is a growing concern in the medical community. The World Health Organization (WHO) has declared it a global health emergency, and [\u0026hellip;]\nInclude stats Introduce the problem in the clinic Summarise the Weis et al paper Summarise the AMR multimodal paper ","permalink":"//localhost:1313/en/blog/amr_multimodal/amr_multimodal/","summary":"Predicting antimicrobial resistance in clinical settings could have significant impact for patients. Here, we extend the current state of the art by incorporating antibiotic structural information into our models.","title":"Multimodal learning in clinical proteomics"},{"content":"General impressions Last December, I had the pleasure of attending the 2023 edition of the Neural Information Processing Systems (NeurIPS) conference in New Orleans. It was my first time in such a large venue, and I must admit I was slightly overwhelmed by the number of people and the sheer amount of information available. However, I got a lot of insight from the several interesting talks and poster sessions I managed to attend, and I would like to share some of my favourite snippets with whoever is interested. Below the three papers I found most interesting for my current research, as well as a bonus track on an amazing project that calls back to my PhD days.\nContrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series The first paper in the list introduces COMET, a contrastive learning framework to learn representations of medical time series in a self-supervised manner. The main contribution the authors present is a hierarchical training scheme for time series encoders that has four main contrastive loss terms, aiming to contrast single observations, trials, samples, and patients, exploiting and learning from different levels of data consistency that may be lost otherwise. For all levels, they use variants of the InfoNCE (Information Noise Contrastive Estimation) loss, where positive and negative pairs are sampled using a masking-based augmentation procedure.\nAs a recap, let\u0026rsquo;s start with the regular InfoNCE loss. Given a set of samples (/x_i/) and a different set /(x_j/), the InfoNCE loss is defined as:\n$$ \\mathcal{L}_{\\text{InfoNCE}} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(x_i, x_i^+))}{\\sum_{j=1}^{K} \\exp(\\text{sim}(x_i, x_j^-))} $$where \\(x_i^+\\) is a positive sample, \\(x_j^-\\) a negative sample, and \\(\\text{sim}\\) a similarity function (typically the dot product between the embeddings or cosine similarity, but it could be anything else). The idea is to maximise the similarity between the encoder and projection head for positive pairs, while minimising it for negative pairs (notice the \\(-\\) sign!). But how do we define positive and negative pairs? Well, this is where COMET shines\u0026hellip;\nThe four levels I mentioned above are defined as follows:\nObservation level: First, augmented observations of the same time point are treated as positive pairs \\((x_{i,t}, \\tilde{x}_{i,t})\\), while real and augmented observations of different time points are treated as negative \\((x_{i,t}, x_{i,-t}\\) and \\((x_{i,t}, \\tilde{x}_{i,-t})\\), where \\(x\\) is a given sample, \\(\\tilde{x}\\) an augmented sample, \\(i\\) the sample index, and \\(t\\) the time index.\nSample level: Second, augmented observations of the same sample are treated as positive pairs \\((x_{i}, \\tilde{x}_{i})\\), while real and augmented observations of different samples are treated as negative \\((x_{i}, x_{j}\\) and \\((x_{i}, \\tilde{x}_{j})\\).\nTrial level: We now take subsets of each time series into account for memory purposes, and group observations belonging to the same subset in to trials. We apply the same basic loss, but to an aggregated representation of the entire trial instead of the observation-level embeddings. Of all four, it\u0026rsquo;s the only one that comes as an artefact of hardware limitations, rather than from real consistencies in the data themselves. It\u0026rsquo;s unclear if this would be necessary in a more powerful hardware setup.\nPatient level: last but not least, we repeat step 3, but now with trials grouped into belonging to the same patient or not. Different time series sampled from the same individuals are regarded as positive pairs, and as negative otherwise. This is really interesting because it\u0026rsquo;s the only level that takes into account the patient\u0026rsquo;s identity, which can be crucial for generalisation in medical data.\nBy training the encoder with these four levels of contrastive loss, the authors aim to learn representations that are invariant to the different levels of data consistency.\nThe main workflow looks like this (check the paper for more details!):\nThe experiments in the paper focus on regularly sampled EEG data with little to no missing values. However, their contrastive framework seems like a great starting point for multi-level learning for irregularly sample data (such as ICU time series) as well. They beat several time series self-supervised baselines in detecting myocardial infarction and Parkinson\u0026rsquo;s disease.\nTime Series as Images: Vision Transformer for Irregularly Sampled Time Series We now move to a paper that caught my attention because of the lateral thinking in place. Time Series as Images presents a peculiar way of processing irregularly sampled time series for supervised learning, based on plotting the collected values as images and processing them using a pretrained vision swin transformer. The authors run several unorthodox experiments on how different styles of plotting influence performance, such as markers, interpolation, variable order, and colours. Moreover, they include several experiments on intensive care unit data, and provide direct comparisons with many baselines, including SeFT from the Borgwardt lab where I work. They reach SOTA performance in various tasks, including sepsis and mortality prediction on physionet 2019 and 2012 challenge data, respectively. They train the models with a simple binary cross-entropy loss, while upsampling the minority class. Interestingly, they also included static information (age, height, weight, sex, demographics) as a paragraph embedded with an encoder-only language model. Time series and static text embeddings were concatenated prior to classification. Truly unorthodox, but it seems to work!\nThe main workflow looks like this (check the paper for more details!):\nAll in all, this article clearly demonstrates the generalising capabilities of large vision models, but whether their ideas can be extended to make generalizable time series models remains to be seen.\nAn Iterative Self-Learning Framework for Medical Domain Generalization Finally, the third work in this summary presents an approach to mitigate distribution shift in electronic health record (EHR) data, named SLDG (Self-Learning Domain Generalization). In a nutshell, the approach starts by grouping the features semantically in different classes (such as statics, symptoms, treatments, and medical history). Each feature subset is embedded with a trained encoder to an individual latent space, and a series of latent domains are retrieved for each modality using hierarchical clustering, with the number of clusters selected automatically based on the silhouette score. This makes it easier to unravel really specific clusters as the intersection of not-so-rare groups of specific features (such as a cluster of older male patients with a history of smoking and type 2 diabetes, which can be decomposed in older, male, smoking, and type 2 diabetes). Lastly, individual classifiers for a given target variable are trained for each of these feature classes, with clusters recomputed every 20 epochs.\nTheir experiments focus on 15-day readmission and mortality prediction on both MIMIC-IV and eICU, with data splits maximising the temporal and spatial gaps between samples, the latter based on the geographical location of the hospitals. They beat several domain generalization baselines on all tasks and metrics, which sounds promising. Interestingly, they do not provide generalization metrics between eICU and MIMIC-IV; only within each dataset individually.\nAll in all, this seems like a smart approach to efficiently group representations based on prior knowledge about the features, which can have a positive impact on domain shift.\nBonus track: AmadeusGPT: a natural language interface for interactive animal behavioral analysis During my PhD, I worked a lot with motion-tracking data coming from animal experiments. The absolute state-of-the-art, both in terms of performance and user support, for motion tracking in biology, is DeepLabCut. However, the main software is not the most user-friendly, and it can be quite cumbersome to use for wet-lab biologists. This is where AmadeusGPT comes in. By leveraging several milestones from the Mathis lab, such as DLC super animal (a model that enables zero-shot tracking), object segmentation using SAM, and API calls to ChatGPT, the authors (also the DeepLabCut team!) present a natural language interface for interactive animal behavioral analysis, where the user can ask questions about the data in plain English, and the system will return the relevant information.\nAt the time of writing, the system is available upon request (which is understandable given the pricing of large-volume ChatGPT API calls), but it seems like a great step forward in making cutting-edge technology more accessible to the general public. I can\u0026rsquo;t wait to see how this project evolves!\n","permalink":"//localhost:1313/en/blog/neurips23/neurips23/","summary":"My impressions, thoughts, and favourite papers on the 2023 edition of NeurIPS, which I attended in person in New Orleans.","title":"Neural Information Processing Systems 2023"},{"content":"","permalink":"//localhost:1313/en/pubs/liver_dvp/","summary":"","title":"Single cell spatial proteomics maps human liver zonation patterns and their vulnerability to fibrosis"},{"content":"","permalink":"//localhost:1313/en/pubs/conformal_amr/","summary":"","title":"Detecting antimicrobial resistance through MALDI-TOF mass spectrometry with statistical guarantees using conformal prediction"},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_metabolism/","summary":"","title":"Sex-specific fear acquisition following early life stress is linked to amygdala and hippocampal purine and glutamate metabolism"},{"content":"","permalink":"//localhost:1313/en/pubs/phd_thesis/","summary":"","title":"Deep clustering of animal motion tracking data: software development and applications to psychiatric preclinical research"},{"content":" You have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\nMy name is Lucas Miranda. I was born and raised in Buenos Aires, Argentina, where I studied molecular biology and bioinformatics at the University of Buenos Aires. I recently completed my doctorate at the Technical University of Munich, directed by Bertram Müller-Myhsok at the Max Planck Institute of Psychiatry. If you\u0026rsquo;re interested, you can read my thesis here.\nSince June 2023, I work as a postdoctoral researcher at the Max Planck Institute of Biochemistry in Munich, Germany, as part of the Machine Learning and Systems Biology research group. My current research focuses on representation learning of biological and medical data, with a particular interest in clinical proteomics and medical time series, including electronic health records and intensive care unit trajectories.\nWe live in exciting times, where machine learning models are becoming better by the day in many domains. My goal is to contribute my bit to the development of this technology in the medical field, which can ultimately benefit us all. Thank you for tuning in, and I hope you find something useful.\n","permalink":"//localhost:1313/en/about/","summary":"\u003chr\u003e\n\u003cp\u003eYou have reached my digital home. Here you\u0026rsquo;ll find content about my professional life, including scientific publications, projects, software, and (some) general interest blog entries.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eMy name is Lucas Miranda. I was born and raised in \u003ca href=\"https://www.youtube.com/watch?v=Pb9Hv9lw5Tw\"\u003eBuenos Aires, Argentina\u003c/a\u003e, where I studied molecular biology and bioinformatics at the \u003ca href=\"https://www.uba.ar/\"\u003eUniversity of Buenos Aires\u003c/a\u003e. I recently completed my doctorate at the \u003ca href=\"https://www.tum.de/en/\"\u003eTechnical University of Munich\u003c/a\u003e, directed by Bertram Müller-Myhsok at the \u003ca href=\"https://www.psych.mpg.de/1495975/mueller_myhsok\"\u003eMax Planck Institute of Psychiatry\u003c/a\u003e. If you\u0026rsquo;re interested, you can read my thesis \u003ca href=\"https://mediatum.ub.tum.de/?id=1713444\"\u003ehere\u003c/a\u003e.\u003c/p\u003e","title":"About me "},{"content":"","permalink":"//localhost:1313/en/pubs/early_life_stress_hippocampus/","summary":"","title":"Early life adversity shapes social subordination and cell type–specific transcriptomic patterning in the ventral hippocampus"},{"content":"","permalink":"//localhost:1313/en/pubs/multimodal_amr/","summary":"","title":"Multimodal learning in clinical proteomics: enhancing antimicrobial resistance prediction models with chemical information"},{"content":"","permalink":"//localhost:1313/en/pubs/social_beh_ns_review/","summary":"","title":"Advancing social behavioral neuroscience by integrating ethology and comparative psychology methods through machine learning"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_ncomms/","summary":"","title":"Automatically annotated motion tracking identifies a distinct social behavioral profile following chronic social defeat stress"},{"content":"","permalink":"//localhost:1313/en/pubs/deepof_joss/","summary":"","title":"DeepOF: A Python package for supervised and unsupervised pattern recognition in mice motion tracking data"},{"content":"","permalink":"//localhost:1313/en/pubs/mirnas_uba/","summary":"","title":"miRNAs associated with endoplasmic reticulum stress and unfolded protein response during decidualization"},{"content":"","permalink":"//localhost:1313/en/pubs/stress_commentary/","summary":"","title":"Increasing resolution in stress neurobiology: from single cells to complex group behaviors"},{"content":"","permalink":"//localhost:1313/en/pubs/fmri_review/","summary":"","title":"Systematic review of functional MRI applications for psychiatric disease subtyping"},{"content":"","permalink":"//localhost:1313/en/pubs/mips_uba/","summary":"","title":"Comprehensive identification of pathogenic gene variants in patients with neuroendocrine disorders"},{"content":"","permalink":"//localhost:1313/en/pubs/nfkb_uba/","summary":"","title":"The transcription factor NF-kB mediates thyrotropin-stimulated expression of thyroid differentiation markers"}]